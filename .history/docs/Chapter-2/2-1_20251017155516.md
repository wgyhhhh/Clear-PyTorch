### Tensor的简介

在前一章中，我们介绍了一个基础的神经网络架构。一般而言，深度学习模型以某种特定形式接收输入数据（例如图像或文本），并将其映射为另一种形式的输出，如类别标签、数值结果或生成文本。换言之，神经网络的核心目标在于构建一个能够将原始数据转化为有效表征的系统。举例来说，当输入一张小狗的图像时，网络可能提取其整体轮廓与典型颜色等关键特征，从而正确输出“黄色小狗”这一标签，而非错误地将其识别为猫或黑色犬只。

在现实世界中，诸如温度、距离或速度等物理量通常为连续实数，理论上可达到任意精度。为在计算机中表示这些数值，我们采用浮点数（floating-point numbers）进行有限精度的近似。因此，构建深度学习系统的关键步骤之一，是将真实世界的数据编码为模型可处理的数值形式，并将模型输出解码为人类可理解的结果。

在将数据转换为浮点数并输入网络之前，我们必须深入理解 PyTorch 中数据的存储与处理机制。本节将重点讨论这一基础内容。

PyTorch 提供了一种核心的数据结构：张量（Tensor）。尽管其名称与数学、物理及工程领域的“张量”概念相同，但在深度学习语境中，张量实质上是向量与矩阵向任意维度的推广。熟悉 NumPy 的读者可能会问：为何不直接使用 NumPy 数组？关键在于 PyTorch 张量支持 GPU 加速计算，这对于现代深度学习的大规模运算至关重要。值得说明的是，PyTorch 与 NumPy 之间具备高效的互转机制，从而能够与 SciPy、scikit-learn 等科学计算库无缝集成。

在接下来的章节中，我们将系统介绍如何对高维张量执行常用操作，并详细阐述其与 NumPy 数组的转换方法及 GPU 加速的实际应用。
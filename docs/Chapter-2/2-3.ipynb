{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565eff6c",
   "metadata": {},
   "source": [
    "### 2.3.1 单个张量的函数运算\n",
    "\n",
    "在2.2节中，我们介绍了一些基本的张量的基本操作。在撰写深度学习项目时，我们往往会遇到另一类操作-张量的运算。例如，对于张量做四则运算、线性变换。这些基础操作可以通过PyTorch中的一些函数实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c030acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2919, 0.8279, 0.9562, 0.5749, 0.2137],\n",
       "        [0.8654, 0.7772, 0.9770, 0.4642, 0.5791],\n",
       "        [0.7826, 0.7781, 0.9563, 0.4069, 0.6980],\n",
       "        [0.2484, 0.9545, 0.0606, 0.8718, 0.4652]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.rand(4, 5)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69add818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5403, 0.9099, 0.9778, 0.7582, 0.4622],\n",
       "        [0.9303, 0.8816, 0.9884, 0.6813, 0.7610],\n",
       "        [0.8846, 0.8821, 0.9779, 0.6379, 0.8354],\n",
       "        [0.4984, 0.9770, 0.2461, 0.9337, 0.6821]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(t1) # 张量的平方根"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6e25871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2919, 0.8279, 0.9562, 0.5749, 0.2137],\n",
       "        [0.8654, 0.7772, 0.9770, 0.4642, 0.5791],\n",
       "        [0.7826, 0.7781, 0.9563, 0.4069, 0.6980],\n",
       "        [0.2484, 0.9545, 0.0606, 0.8718, 0.4652]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "247b56c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5403, 0.9099, 0.9778, 0.7582, 0.4622],\n",
       "        [0.9303, 0.8816, 0.9884, 0.6813, 0.7610],\n",
       "        [0.8846, 0.8821, 0.9779, 0.6379, 0.8354],\n",
       "        [0.4984, 0.9770, 0.2461, 0.9337, 0.6821]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.sqrt_()  # 在原张量上进行操作\n",
    "t1  # 张量的值被改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33bc50a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.4463)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t1)  # 张量所有元素求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11dc138d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8536, 3.6506, 3.1903, 3.0111, 2.7407])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t1, 0) # 对第0维的元素求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2bfce21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.4463)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t1, [0,1]) # 对第0维和第1维的元素求和  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2fd72744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7723)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t1, [0,1]) # 对第0维和第1维的元素求平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9deb252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7723)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mean([0,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03d68d",
   "metadata": {},
   "source": [
    "对于大多数我们常用的函数，一般有两种调用方式。一种可以是使用张量的内置方法，另一种则是使用torch自带的函数，这两种的操作结果均相同。对于上述求平方根和求平均值，也可以使用张量自带的方法进行。在上面代码中我们可以看到，很多张量自带的方法都有一个“下划线”版本，该方法会直接改变调用方法的张量的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fab95",
   "metadata": {},
   "source": [
    "### 2.3.2 多个张量的函数运算\n",
    "\n",
    "除了前面对单个张量作为参数进行操作外，还有以两个张量作为参数的操作。比如，两个形状相同的张量之间的逐元素的四则运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2fcc684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:\n",
      " tensor([[0.1343, 0.4706, 0.7728],\n",
      "        [0.4805, 0.5326, 0.3688]])\n",
      "t2:\n",
      " tensor([[0.5517, 0.4579, 0.3883],\n",
      "        [0.4306, 0.2069, 0.7323]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(2 ,3)\n",
    "t2 = torch.rand(2 ,3)\n",
    "print(\"t1:\\n\", t1)\n",
    "print(\"t2:\\n\", t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bb06287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6860, 0.9285, 1.1611],\n",
       "        [0.9111, 0.7395, 1.1011]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.add(t2)  # 逐元素相加，不改变参与运算张量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e644742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6860, 0.9285, 1.1611],\n",
       "        [0.9111, 0.7395, 1.1011]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2  # 逐元素相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ecac064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4173,  0.0127,  0.3845],\n",
       "        [ 0.0499,  0.3257, -0.3636]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.sub(t2)  # 逐元素相减，不改变参与运算张量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca603557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4173,  0.0127,  0.3845],\n",
       "        [ 0.0499,  0.3257, -0.3636]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1-t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ffc2913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0741, 0.2155, 0.3001],\n",
       "        [0.2069, 0.1102, 0.2701]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mul(t2)  # 逐元素相乘，不改变参与运算张量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6f3ab5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0741, 0.2155, 0.3001],\n",
       "        [0.2069, 0.1102, 0.2701]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1*t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5680c787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2435, 1.0277, 1.9902],\n",
       "        [1.1159, 2.5742, 0.5036]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.div(t2)  # 逐元素相除，不改变参与运算张量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32fc75da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2435, 1.0277, 1.9902],\n",
       "        [1.1159, 2.5742, 0.5036]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1/t2 # 逐元素相除"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f0cd7",
   "metadata": {},
   "source": [
    "同样的，这些内置的方法也有“下划线”版本，可以改变调用方法中张量的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e779b38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6860, 0.9285, 1.1611],\n",
       "        [0.9111, 0.7395, 1.1011]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.add_(t2)  # 逐元素相加，改变调用方法中张量的值\n",
    "t1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b0a79",
   "metadata": {},
   "source": [
    "### 2.3.3 张量的极值和排序\n",
    "\n",
    "我们通常在编写代码时，要获得张量(沿着某个维度)的最大值或最小值，以及这些值所在的位置，此时我们便可以使用`max`和`min`。通过传入具体的维度，同时返回该维度最大和最小值的位置，以及对应最大值和最小值组成的元组(Tuple)。\n",
    "\n",
    "同时我们将介绍排序函数`sort`(默认顺序是从小到大，如果要从大到小则需要设置参数为`descending=True`)，同样传入具体需要排序的维度，将返回排序后的张量，以及对应排序后元素在原始张量上的位置。如果想要知道原始张量的元素沿着某个维度排第几位，只需要对相应排序后的元素在原始张量上的位置进行再次排序，得到新位置的值即为原始张量沿着该方向进行大小排序后的序号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02e90b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9740,  1.9660, -0.4722, -0.6993],\n",
       "        [-0.7022, -1.5544, -0.2736,  1.4672],\n",
       "        [ 0.0223,  1.5819, -0.0632, -0.2258]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(3, 4) # 创建一个3行4列的张量\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4286354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t, dim=0)  # 返回沿着第0个维度，极大值所在的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79f5303b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmin(dim=0)  # 返回沿着第0个维度，极小值所在的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "59425c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.9660, 1.4672, 1.5819]),\n",
       "indices=tensor([1, 3, 1]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(t, dim=-1)  # 返回沿着最后一个维度，极大值及其位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c700129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([-0.7022, -1.5544, -0.4722, -0.6993]),\n",
       "indices=tensor([1, 1, 0, 0]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.min(0)  # 返回沿着第0个维度，极小值及其位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d9d89db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-0.6993, -0.4722,  0.9740,  1.9660],\n",
       "        [-1.5544, -0.7022, -0.2736,  1.4672],\n",
       "        [-0.2258, -0.0632,  0.0223,  1.5819]]),\n",
       "indices=tensor([[3, 2, 0, 1],\n",
       "        [1, 0, 2, 3],\n",
       "        [3, 2, 0, 1]]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sort(-1)  # 沿着最后一个维度排序，返回排序后的张量和张量元素在该维度的原始位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e55c8",
   "metadata": {},
   "source": [
    "### 2.3.4 矩阵的乘法\n",
    "\n",
    "除了四则运算，最大和最小值运算，以及排序外。两个张量作为参数进行矩阵乘法(线性变换)也非常重要。有以下几种方法可以实现矩阵乘法运算。\n",
    "- `torch.mm`\n",
    "- 张量内置的`mm`方法\n",
    "- python中固定的运算符号`@`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f812925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[ 1.0309,  0.2665, -0.4899,  1.0726],\n",
      "        [ 0.3234,  0.5080, -0.6658,  0.0648],\n",
      "        [ 0.5392, -0.0989,  1.4689,  2.0395]])\n",
      "b:\n",
      " tensor([[ 0.5076, -0.7805, -0.3212],\n",
      "        [ 1.0104,  0.5980, -1.6927],\n",
      "        [ 1.3665,  0.0965,  1.3819],\n",
      "        [-1.0371,  0.2984,  0.6112]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(4,3)\n",
    "print(\"a:\\n\", a)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a35f9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9893, -0.3725, -0.8038],\n",
       "        [-0.2997,  0.0065, -1.8443],\n",
       "        [ 0.0659,  0.2704,  3.2706]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(a, b)  # 矩阵乘法，返回一个新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4d60448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9893, -0.3725, -0.8038],\n",
       "        [-0.2997,  0.0065, -1.8443],\n",
       "        [ 0.0659,  0.2704,  3.2706]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mm(b)  # 矩阵乘法，返回一个新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eba9a29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9893, -0.3725, -0.8038],\n",
       "        [-0.2997,  0.0065, -1.8443],\n",
       "        [ 0.0659,  0.2704,  3.2706]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b  # 矩阵乘法，返回一个新的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e21ce6",
   "metadata": {},
   "source": [
    "在深度学习项目中，经常用到的三维张量的数据，一般来说，第一个维度是批次大小。因此，可以将三维张量看作是一个批次数量的矩阵叠加在一起。在这种情况下，如果两个张量做矩阵乘法，一般情况是沿着批次方向分别对每个矩阵做成发，最后将所有乘积的结果整合在一起。如果是大小`b×m×k`和`b×k×n`的张量相乘，那么结果应该是一个`b×m×n`的张量。也就是说两个张量的第一维是相等的，然后第一个张量的第三维和第二个张量的第二维度要求一样，对于剩下的则不做要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc5f7834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[[-2.2252,  0.1116,  0.4163, -1.2188],\n",
      "         [ 0.1370, -0.5574,  0.5923, -0.5445],\n",
      "         [ 1.1473,  0.3684, -1.3186, -0.7308]],\n",
      "\n",
      "        [[ 0.2357,  0.8854,  1.3761,  0.5599],\n",
      "         [ 0.2456, -0.0559, -1.3316,  0.5059],\n",
      "         [-0.1422, -0.7040,  0.9162,  0.8699]]])\n",
      "b:\n",
      " tensor([[[-0.4713, -0.4337, -2.4821],\n",
      "         [ 0.4874,  0.6579,  0.1481],\n",
      "         [ 0.7768, -1.3407, -0.4709],\n",
      "         [-0.8209,  1.7036, -0.6241]],\n",
      "\n",
      "        [[-0.8176,  0.8839, -1.0987],\n",
      "         [ 1.1911, -1.8381, -1.1256],\n",
      "         [ 0.7311, -0.7400,  0.9529],\n",
      "         [ 0.1100, -0.3319,  0.5146]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(2, 4, 3)\n",
    "print(\"a:\\n\", a)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56041faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4270, -1.5959,  6.1043],\n",
       "         [ 0.5709, -2.1479, -0.3616],\n",
       "         [-0.7855,  0.2677, -1.7163]],\n",
       "\n",
       "        [[ 1.9294, -2.6231,  0.3439],\n",
       "         [-1.1853,  1.1373, -1.2154],\n",
       "         [ 0.0432,  0.2017,  2.2692]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(a, b)  # 批量矩阵乘法，返回一个新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "453379ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4270, -1.5959,  6.1043],\n",
       "         [ 0.5709, -2.1479, -0.3616],\n",
       "         [-0.7855,  0.2677, -1.7163]],\n",
       "\n",
       "        [[ 1.9294, -2.6231,  0.3439],\n",
       "         [-1.1853,  1.1373, -1.2154],\n",
       "         [ 0.0432,  0.2017,  2.2692]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bmm(b)  # 批量矩阵乘法，返回一个新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ed526cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.4270, -1.5959,  6.1043],\n",
       "         [ 0.5709, -2.1479, -0.3616],\n",
       "         [-0.7855,  0.2677, -1.7163]],\n",
       "\n",
       "        [[ 1.9294, -2.6231,  0.3439],\n",
       "         [-1.1853,  1.1373, -1.2154],\n",
       "         [ 0.0432,  0.2017,  2.2692]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d218ff",
   "metadata": {},
   "source": [
    "对于更大维度的张量的乘积，往往要决定张量元素乘积的结果需要沿着哪些维度进行求和，这个过程称为张量收缩(`Contraction`)，这时候需要引入爱因斯坦求和约定(`einsum`)，`einsum`提供了一套既简洁又优雅的规则，可实现包括但不限于：向量内积、向量外积、矩阵乘法、转置和张量收缩等张量操作，熟练运用`einsum`可以很方便的实现复杂的张量操作，而且不容易出错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f960e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      " tensor([[[ 1.3017e+00,  1.5896e-01, -4.0868e-01],\n",
      "         [ 1.7325e+00,  3.6240e+00,  3.9897e+00],\n",
      "         [-2.2171e+00, -1.5945e+00, -3.5344e-01]],\n",
      "\n",
      "        [[ 1.9188e-01, -1.1419e+00,  1.5174e+00],\n",
      "         [-2.3588e+00,  3.0826e+00, -1.8356e+00],\n",
      "         [-8.6201e-01,  1.8974e+00,  1.9701e-03]]])\n",
      "d:\n",
      " tensor([[[ 1.3017e+00,  1.5896e-01, -4.0868e-01],\n",
      "         [ 1.7325e+00,  3.6240e+00,  3.9897e+00],\n",
      "         [-2.2171e+00, -1.5945e+00, -3.5344e-01]],\n",
      "\n",
      "        [[ 1.9188e-01, -1.1419e+00,  1.5174e+00],\n",
      "         [-2.3588e+00,  3.0826e+00, -1.8356e+00],\n",
      "         [-8.6201e-01,  1.8974e+00,  1.9701e-03]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(2, 4, 3)\n",
    "c = a.bmm(b)  # 批量矩阵乘法，返回一个新的张量\n",
    "print(\"c:\\n\", c)\n",
    "d = torch.einsum('ijk,ikl->ijl', a, b)  # 使用einsum实现批量矩阵乘法，与前面的结果一致\n",
    "print(\"d:\\n\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f1ece",
   "metadata": {},
   "source": [
    "其中需要重点关注的是`einsum`的第一个参数`ijk,ikl->ijl`(下面以`equation`表示)，该字符串表示了输入和输出张量的维度。`equation`中的箭头左边表示输入张量，以逗号分割每个输入张量，箭头右边则表示输出张量。表示维度的字符只能是26个英文字母`a-z`。\n",
    "\n",
    "而`einsum`的第二个参数表示实际的输入张量列表，其数量要与`equation`中的输入数量对应。同时对应每个张量的子`equation`的字符个数要与张量的真实维度对应，比如`ijk,ikl->ijl`表示输入和输出张量都是三维的。`equation`中的字符也可以理解为索引，就是输出张量的某个位置的值，是怎么从输入张量中得到的，比如上面矩阵乘法的`d`的某个点`d[i, j, l]`的值是通过`a[i, j, k]`和`b[i, k, l]`沿着`k`这个维度做内积得到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4fbcd",
   "metadata": {},
   "source": [
    "### 2.3.5 张量的拼接和分割\n",
    "\n",
    "在实际场景中，我们经常会碰到的一张情况是把不同的张量按照某一个维度组合在一起，或者把一个张量按照一定的形状进行分割，这时候就需要用到张量的组合和分割函数。主要有以下几种函数。\n",
    "- `torch.stack`\n",
    "传入的张量列表，并同时指定并创建一个维度，把列表的张量沿着该维度堆叠起来，并返回堆叠以后的张量。列表中的所有张量的大小必须一致。\n",
    "- `torch.cat`\n",
    "传入的张量列表，并同时指定某一个维度，把列表中的向量沿着该维度堆叠起来，并返回堆叠以后的张量。列表中的所有张量除指定的维度外，其他维度的大小必须一致。其与前者`torch.stack`的区别是，`torch.stack`会新建一个维度。\n",
    "- `torch.split`\n",
    "传入被分割的张量、分割后维度的大小和分割的维度。如果传入整数，则沿着分割维度分割为成好几段，每段沿着分割维度的大小是传入的整数；如果传入整数列表，则按照列表整数的大小来分割维度。\n",
    "- `torch.chunk`\n",
    "传入被分割的张量、分割后张量的数量和分割的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e81c039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = torch.randn(2, 3)\n",
    "a2 = torch.randn(2, 3)\n",
    "a3 = torch.randn(2, 3)\n",
    "a4 = torch.randn(2, 3)# 随机生成四个形状为(2, 3)的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "16947995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a1, a2, a3, a4], dim=-1).shape  # 在最后一个维度进行拼接，结果形状为(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "59ebdd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a1, a2, a3, a4], dim=0).shape  # 在第0个维度进行拼接，结果形状为(8, 3)\n",
    "torch.cat([a1, a2, a3, a4], dim=1).shape  # 在第1个维度进行拼接，结果形状为(2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4790f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7919],\n",
       "         [-2.1460],\n",
       "         [-0.2391]]),\n",
       " tensor([[-0.0118,  0.6845],\n",
       "         [-0.0736, -0.8396],\n",
       "         [ 0.6493, -0.0980]]),\n",
       " tensor([[ 0.4018,  0.1497, -0.2742],\n",
       "         [-1.1010, -1.2802,  0.7435],\n",
       "         [-0.3161, -0.3086,  0.7839]]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3, 6) # 随机生成一个形状为(3, 6)的张量\n",
    "a.split([1,2,3], dim=-1)  # 沿着最后的维度将张量分割为三个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4fb07990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7919, -0.0118,  0.6845],\n",
       "         [-2.1460, -0.0736, -0.8396],\n",
       "         [-0.2391,  0.6493, -0.0980]]),\n",
       " tensor([[ 0.4018,  0.1497, -0.2742],\n",
       "         [-1.1010, -1.2802,  0.7435],\n",
       "         [-0.3161, -0.3086,  0.7839]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(3, dim = -1) # 把张量沿着最后一个维度分割，分割大小为3，输出的张量大小均为(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c476047d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7919, -0.0118],\n",
       "         [-2.1460, -0.0736],\n",
       "         [-0.2391,  0.6493]]),\n",
       " tensor([[ 0.6845,  0.4018],\n",
       "         [-0.8396, -1.1010],\n",
       "         [-0.0980, -0.3161]]),\n",
       " tensor([[ 0.1497, -0.2742],\n",
       "         [-1.2802,  0.7435],\n",
       "         [-0.3086,  0.7839]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.chunk(3, dim = -1) # 把张量沿着最后一个维度分割为3个张量，大小均为(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4e3a5",
   "metadata": {},
   "source": [
    "### 2.3.6 张量维度的扩增和压缩\n",
    "\n",
    "我们经常会在实际情况中，沿着张量的某个方向做扩增`Expand`或对张量进行压缩`Squeeze`，这两种情况与张量大小等于1的维度有关。对一个张量来说，可以任意添加一个大小为1的维度，而不改变张量的数据，因为张量的大小等于所有维度大小的乘积，大小为1维度并不改变张量的大小。于是我们可以在张量中添加任意数目大小为1的维度。主要有以下几种函数。\n",
    "- `unsqueeze`\n",
    "- `squeeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4faa4e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "88d312ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1).shape  # 在最后一个维度增加一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "217fa792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1).unsqueeze(-1).shape  # 继续扩增一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f66e024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d9a393b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.squeeze().shape # 压缩所有大小为1的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b9cbb",
   "metadata": {},
   "source": [
    "### 2.3.7 张量的广播\n",
    "\n",
    "张量的扩增`Expand`有助于实现张量的另一种功能，即张量的广播`(Broadcast)`。在张量的运算中会碰到另外一种情况，即两个不同维度的张亮之间做四则运算，且两个张量的某些维度相等。显然，如果按照张量的四则运算的定义，两个不同维度的张量不能进行四则运算，为了能够让它们进行计算，首先需要使用`unsqueeze`方法对维度数目较小的张量进行扩增，完成扩增维度的两个张量必须能够在维度上对齐。\n",
    "\n",
    "举例说明，假设一个张量的大小为`3×4×5`，另外一个张量大小为`3×5`，为了能够让两个张量进行四则运算，需要把第二个张量的形状扩增为`3×1×5`，这样两个张量就可以实现对齐。关于大小为`3×4×5`的张量如何与大小`3×1×5`的张量进行四则运算，其定义是将`3×1×5`的张量沿着第二个维度复制`4`次，使之成为`3×4×5`的张量，这样就可以进行元素间一一对应的运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "38fff073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1:\n",
      " tensor([[[ 1.0493, -0.5901,  0.2419, -0.0474, -0.7105],\n",
      "         [ 1.3368, -0.8858,  1.1853,  0.5034, -0.5362],\n",
      "         [-1.0567, -1.6474,  0.1150, -1.0403, -0.8554],\n",
      "         [-2.0884,  0.7160, -1.3028,  0.5399,  0.8133]],\n",
      "\n",
      "        [[-0.4069, -2.6390,  0.0572, -0.8220,  1.1420],\n",
      "         [ 0.0190, -2.2368, -0.3912, -0.9439, -1.3652],\n",
      "         [-2.0559, -0.9280,  1.2714, -0.6071,  0.7605],\n",
      "         [ 0.2657,  0.0755, -2.4774,  0.2878, -0.6732]],\n",
      "\n",
      "        [[ 0.2958,  0.8898, -0.6109, -0.2262, -0.1938],\n",
      "         [-1.6564,  1.6879, -0.1754,  2.4246,  0.6358],\n",
      "         [ 0.8858, -1.2357, -0.5082,  0.9960,  0.2550],\n",
      "         [ 0.2759,  0.5073, -0.5185, -2.1496,  0.4462]]])\n",
      "a2:\n",
      " tensor([[-0.0156, -0.9247,  1.0962, -0.8086, -1.1293],\n",
      "        [ 0.8451, -0.0890,  0.1846, -0.8928,  0.0059],\n",
      "        [-1.1582,  0.8007, -0.2943,  1.5512,  1.6541]])\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.randn(3, 4, 5)\n",
    "print(\"a1:\\n\", a1)\n",
    "a2 = torch.randn(3, 5)\n",
    "print(\"a2:\\n\", a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "117696b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 shape: torch.Size([3, 4, 5])\n",
      "a2 shape: torch.Size([3, 1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0337, -1.5148,  1.3380, -0.8560, -1.8398],\n",
       "         [ 1.3212, -1.8105,  2.2815, -0.3053, -1.6655],\n",
       "         [-1.0723, -2.5722,  1.2112, -1.8489, -1.9847],\n",
       "         [-2.1040, -0.2087, -0.2066, -0.2687, -0.3160]],\n",
       "\n",
       "        [[ 0.4381, -2.7280,  0.2419, -1.7148,  1.1479],\n",
       "         [ 0.8641, -2.3258, -0.2065, -1.8367, -1.3593],\n",
       "         [-1.2108, -1.0170,  1.4560, -1.4999,  0.7665],\n",
       "         [ 1.1107, -0.0134, -2.2928, -0.6050, -0.6673]],\n",
       "\n",
       "        [[-0.8623,  1.6905, -0.9052,  1.3251,  1.4603],\n",
       "         [-2.8145,  2.4886, -0.4697,  3.9759,  2.2899],\n",
       "         [-0.2724, -0.4350, -0.8025,  2.5473,  1.9091],\n",
       "         [-0.8823,  1.3080, -0.8128, -0.5984,  2.1003]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = a2.unsqueeze(1)  # 对齐维度\n",
    "print(\"a1 shape:\", a1.shape)\n",
    "print(\"a2 shape:\", a2.shape)\n",
    "a3 = a1 + a2  # 张量广播机制实现加法运算\n",
    "a3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
